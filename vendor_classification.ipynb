{
 "cells": [
  {
<<<<<<< Updated upstream
   "cell_type": "markdown",
   "id": "c9eb4277-01b1-4bb6-89f5-e86ba4148a92",
   "metadata": {},
   "source": [
    "## Remove 'merchantCategory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be7545f-4a67-43fd-aa1f-083879055731",
=======
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8dbbc7-3b6a-44cf-9e3e-7838fd89fd3c",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "C:\\Users\\zianz\\OneDrive\\Documents\\GitHub\\receiptParse\n",
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# # get the current working directory\n",
    "# current_working_directory = os.getcwd()\n",
    "\n",
    "# # print output to the console\n",
    "# print(current_working_directory)\n",
    "\n",
    "# # Directory paths\n",
    "# input_directory = \"data/receipts/json/prompt2\"\n",
    "# output_directory = \"data/receipts/json/json_for_classification\"\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# if not os.path.exists(output_directory):\n",
    "#     os.makedirs(output_directory)\n",
    "\n",
    "# # Function to try reading a file with different encodings\n",
    "# def try_read_file(file_path):\n",
    "#     encodings = ['utf-8', 'latin-1', 'windows-1252']  # Add more if needed\n",
    "#     for encoding in encodings:\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding=encoding) as file:\n",
    "#                 return json.load(file), encoding\n",
    "#         except UnicodeDecodeError:\n",
    "#             continue\n",
    "#     raise ValueError(f\"File {file_path} has an unknown encoding.\")\n",
    "\n",
    "# # Process and save JSON files\n",
    "# for file_name in os.listdir(input_directory):\n",
    "#     file_path = os.path.join(input_directory, file_name)\n",
    "    \n",
    "#     try:\n",
    "#         data, encoding_used = try_read_file(file_path)\n",
    "#     except ValueError as e:\n",
    "#         print(e)\n",
    "#         continue  # Skip this file if encoding issue\n",
    "\n",
    "#     # Remove 'merchantCategory' if it exists\n",
    "#     data['ReceiptInfo'].pop('merchantCategory', None)\n",
    "\n",
    "#     # Save modified data\n",
    "#     output_file_path = os.path.join(output_directory, file_name)\n",
    "#     with open(output_file_path, 'w', encoding=encoding_used) as file:\n",
    "#         json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8f444-ac37-4233-8941-7a8d7091fd80",
   "metadata": {},
   "source": [
    "## Vendor Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e4f9b-db30-45f7-9b94-8a0c31ce6941",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ce5a0c-4f83-48d1-8c28-54b3341742cf",
   "metadata": {},
   "outputs": [
=======
      "Expecting value: line 1 column 1 (char 0)\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
>>>>>>> Stashed changes
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
=======
      "C:\\Users\\zianz\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Cluster 1 (Grocery and Supermarkets): Walmart, ABC STORES, Walmart, Walmart, Walmart, Grain Exchange, Walmart, Dunkin', Dunkin', Walmart, Wal-Mart, Walmart, Dunkin', Walmart, Honolulu Store, Walmart\n",
      "Cluster 2 (Clothing and Apparel): Olive, DRIVER LICENSE HAWAII KA, Ralphs\n",
      "Cluster 3 (Health and Beauty): Store 215 Dir Dane Elder, Minit, PHO 777, 2HI001 See's Candies, Inc, Kahala, Longs Drugs, H Mart, KOZO SUSHI MOILIILI, Foodland, COSTCO, How doers, <UNKNOWN> Drugs, PACS UN, Krispy Kreme, COSTCO, KOZO SUSHI, LEAHI HEALTH MANOA, H-MART, ROSS DRESS FOR LESS, H MART, Panda Express #1866, KOZO SUSHI, BEN FRANKLIN CRAFTS #05614 (3), T&C Surf Designs Ala Moana\n",
      "Cluster 4 (Electronics and Appliances): \n",
      "Cluster 5 (Home and Garden): Store, BEN FRANKLIN CRAFTS #05575, SAFEWAY, Welcome to <UNKNOWN>, Longs, SAFEWAY, SAFEWAY O, Longs, SAFEWAY, THE COUNTER, Longs, sam's club, sam's club, SAFEWAY, SAFEWAY, Oahu, SAFEWAY, Dunkin, SAFEWAY, THE COUNTER, SAFEWAY, University of Hawaii - Manoa Bookstore, SAFEWAY, Baskin Robbins, SAFEWAY, SAFEWAY\n",
      "Cluster 6 (Entertainment and Leisure): \n",
      "Cluster 7 (Restaurants and Food Services): Oahu Honolulu Ala Moana, WHOLE FOODS MARKET, WHOLE FOODS MARKET, Taco Bell 002491, KAHEKA STORE, LILIHA BAKERY, COSTCO WHOLESALE, WHOLE FOODS MARKET, WHOLE FOODS MARKET, Foodland Farms, UNITED STATES POSTAL SERVICE, KAHEKA STORE, Honolulu Intl Airport 0169, Whole Foods Market, COSTCO WHOLESALE, Panda Express #2150, WHOLE FOODS MARKET, KAHEKA STORE, WHOLE FOODS MARKET, Taco Bell 001281, Halal Gyro Kabob House, KAHEKA STORE, McDonald's Restaurant #2331, SELF CHECKOUT KAHEKA STORE\n",
      "                         Category               Vendor Name  \\\n",
      "30           Clothing and Apparel  DRIVER LICENSE HAWAII KA   \n",
      "13           Clothing and Apparel                     Olive   \n",
      "88           Clothing and Apparel                    Ralphs   \n",
      "22       Grocery and Supermarkets                ABC STORES   \n",
      "59       Grocery and Supermarkets                   Dunkin'   \n",
      "..                            ...                       ...   \n",
      "47  Restaurants and Food Services        WHOLE FOODS MARKET   \n",
      "52  Restaurants and Food Services        WHOLE FOODS MARKET   \n",
      "69  Restaurants and Food Services        WHOLE FOODS MARKET   \n",
      "73  Restaurants and Food Services        WHOLE FOODS MARKET   \n",
      "62  Restaurants and Food Services        Whole Foods Market   \n",
      "\n",
      "                                            File Name  \n",
      "30  2a87cf3f-e2cb-4953-b36c-0c8678ffd458_prompttem...  \n",
      "13  18ac5f08-bceb-450f-9de2-85084af2fa13_prompttem...  \n",
      "88  8e7e79a2-1397-4230-bf37-4792e53dac38_prompttem...  \n",
      "22  1ddb89a7-a369-47f0-88af-5461205d1479_prompttem...  \n",
      "59  5e8cbd2e-209d-4efb-a8ec-514077369f7f_prompttem...  \n",
      "..                                                ...  \n",
      "47  4d9529ea-2a57-4d7f-a80e-cba706adbebd_prompttem...  \n",
      "52  50b1e424-ec8c-4179-af9c-0f9c651d0274_prompttem...  \n",
      "69  6bb9c668-43e2-476e-91ce-70ebb87e3875_prompttem...  \n",
      "73  6fba399b-4d30-4eae-bd6b-76e1fb695fc1_prompttem...  \n",
      "62  5ef5df8f-05dc-4a45-9e06-214b3882d4d7_prompttem...  \n",
      "\n",
      "[93 rows x 3 columns]\n"
=======
      "Best Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Best Score: 0.6071428571428571\n",
      "Improved Random Forest Model Accuracy: 0.5454545454545454\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_directory = 'data/receipts/json/json_for_classification'\n",
    "json_files = [f for f in os.listdir(data_directory) if f.endswith('.json')]\n",
    "\n",
    "# Convert Data into Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens.to(device))\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move the output back to CPU\n",
    "\n",
    "vendor_data = []\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(data_directory, json_file), 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "        merchant_name = json_data['ReceiptInfo']['merchant']\n",
    "        address = json_data['ReceiptInfo']['address']\n",
    "        city = json_data['ReceiptInfo']['city']\n",
    "        state = json_data['ReceiptInfo']['state']\n",
    "        tax = json_data['ReceiptInfo']['tax']\n",
    "        receipt_time = json_data['ReceiptInfo']['receiptTime']\n",
    "        \n",
    "        # Skip vendors with \"<UNKNOWN>\" names\n",
    "        if merchant_name == \"<UNKNOWN>\":\n",
    "            continue\n",
    "        \n",
    "        items = json_data['ReceiptInfo']['ITEMS']\n",
    "        \n",
    "        # Create embeddings for merchant name, address, and city\n",
    "        merchant_embedding = get_embedding(merchant_name)\n",
    "        address_embedding = get_embedding(address)\n",
    "        city_embedding = get_embedding(city)\n",
    "        \n",
    "        # Create embeddings for \"description,\" \"quantity,\" \"unitPrice,\" and \"totalPrice\" within \"ITEMS\"\n",
    "        item_embeddings = []\n",
    "        for item in items:\n",
    "            description = item['description']\n",
    "            quantity = item['quantity']\n",
    "            unit_price = item['unitPrice']\n",
    "            total_price = item['totalPrice']\n",
    "            \n",
    "            # Create an embedding for the item details (you can customize this part)\n",
    "            item_details = f\"Description: {description}, Quantity: {quantity}, Unit Price: {unit_price}, Total Price: {total_price}\"\n",
    "            item_embedding = get_embedding(item_details)\n",
    "            item_embeddings.append(item_embedding)\n",
    "        \n",
    "        # Create embeddings for \"state,\" \"tax,\" and \"receiptTime\"\n",
    "        state_embedding = get_embedding(state)\n",
    "        tax_embedding = get_embedding(str(tax))  # Convert tax to a string\n",
    "        receipt_time_embedding = get_embedding(str(receipt_time))\n",
    "        \n",
    "        # Combine all embeddings for this vendor\n",
    "        combined_embedding = np.concatenate(\n",
    "            (merchant_embedding, address_embedding, city_embedding, state_embedding, tax_embedding, receipt_time_embedding, *item_embeddings),\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        vendor_data.append({'merchant': merchant_name, 'embedding': combined_embedding, \"file_name\": json_file})\n",
    "\n",
    "\n",
    "# Create embeddings for the 7 predefined vendor categories\n",
    "categories = [\n",
    "    \"Grocery and Supermarkets\",\n",
    "    \"Clothing and Apparel\",\n",
    "    \"Health and Beauty\",\n",
    "    \"Electronics and Appliances\",\n",
    "    \"Home and Garden\",\n",
    "    \"Entertainment and Leisure\",\n",
    "    \"Restaurants and Food Services\"\n",
    "]\n",
    "\n",
    "category_embeddings = [get_embedding(category) for category in categories]\n",
    "\n",
    "# Calculate cosine similarity between each vendor's combined embedding and category embeddings\n",
    "similarity_matrix = np.zeros((len(vendor_data), len(categories)))\n",
    "\n",
    "for i, vendor in enumerate(vendor_data):\n",
    "    combined_embedding = vendor['embedding']\n",
    "    for j, category_embedding in enumerate(category_embeddings):\n",
    "         # Ensure category_embedding has the same dimension as combined_embedding\n",
    "        if combined_embedding.shape[0] != category_embedding.shape[0]:\n",
    "            category_embedding = np.pad(category_embedding, (0, combined_embedding.shape[0] - category_embedding.shape[0]))\n",
    "        similarity = cosine_similarity([combined_embedding], [category_embedding])\n",
    "        similarity_matrix[i, j] = similarity[0, 0]\n",
    "\n",
    "# Assign each vendor to the category with the highest similarity\n",
    "cluster_labels = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "for cluster_id, category in enumerate(categories):\n",
    "    cluster_vendors = [vendor_data[i]['merchant'] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    print(f\"Cluster {cluster_id + 1} ({category}): {', '.join(cluster_vendors)}\")\n",
    "\n",
    "\n",
    "# Export results to csv\n",
    "result_df = pd.DataFrame(columns=['Category', 'Vendor Name', 'File Name'])\n",
    "\n",
    "for i, vendor in enumerate(vendor_data):\n",
    "    category = categories[cluster_labels[i]]\n",
    "    vendor_name = vendor['merchant']\n",
    "    file_name = vendor['file_name']\n",
    "    result_df = result_df.append({'Category': category, 'Vendor Name': vendor_name, 'File Name': file_name}, ignore_index=True)\n",
    "\n",
    "result_df.sort_values(by=['Category', 'Vendor Name'], inplace=True)\n",
    "\n",
    "result_df.to_csv('vendor_classification.csv', index=False)\n",
    "print(result_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ae09bde-18d7-4546-bf41-02cbc2432e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Category          Vendor Name  \\\n",
      "26           Clothing and Apparel         Krispy Kreme   \n",
      "50           Clothing and Apparel  ROSS DRESS FOR LESS   \n",
      "21     Electronics and Appliances               COSTCO   \n",
      "34     Electronics and Appliances               COSTCO   \n",
      "41     Electronics and Appliances     COSTCO WHOLESALE   \n",
      "..                            ...                  ...   \n",
      "88  Restaurants and Food Services               Ralphs   \n",
      "14  Restaurants and Food Services            SAFEWAY O   \n",
      "0   Restaurants and Food Services                Store   \n",
      "36  Restaurants and Food Services          THE COUNTER   \n",
      "67  Restaurants and Food Services          THE COUNTER   \n",
      "\n",
      "                                            File Name  \n",
      "26  1f78e2ac-bae9-446a-9cef-479d1211583f_prompttem...  \n",
      "50  4e00e015-80b1-482d-b153-6742be0fa1ce_prompttem...  \n",
      "21  1da1edc9-f7dd-47a3-bf76-1af0f4feecb8_prompttem...  \n",
      "34  2d3f179b-ef1a-4486-8c20-7a9323c1c2df_prompttem...  \n",
      "41  3ea4385b-572f-42f7-98ec-d6585a53297d_prompttem...  \n",
      "..                                                ...  \n",
      "88  8e7e79a2-1397-4230-bf37-4792e53dac38_prompttem...  \n",
      "14  18db2bf7-f499-41d0-9a38-e4a543656eb5_prompttem...  \n",
      "0   02a5d5cc-564a-496f-ae09-5ee2004978b7_prompttem...  \n",
      "36  2f3379d0-1556-4308-bad3-164a1c9cc900_prompttem...  \n",
      "67  6b525e9d-0a0e-462e-b12b-cc1f5b74ff7a_prompttem...  \n",
      "\n",
      "[93 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens.to(device))\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Enhanced category descriptions\n",
    "category_descriptions = {\n",
    "    \"Grocery and Supermarkets\": \"Includes vendors selling a wide range of food products, household items, and personal care products.\",\n",
    "    \"Clothing and Apparel\": \"Stores selling clothing, footwear, and fashion accessories for various age groups and styles.\",\n",
    "    \"Health and Beauty\": \"Includes pharmacies, beauty supply stores, and retailers selling health and personal care products.\",\n",
    "    \"Electronics and Appliances\": \"Vendors specializing in consumer electronics, household appliances, and related accessories.\",\n",
    "    \"Home and Garden\": \"Vendors offering home improvement, gardening supplies, furniture, and home decor.\",\n",
    "    \"Entertainment and Leisure\": \"Includes movie theaters, bookstores, hobby shops, and other vendors providing entertainment goods and services.\",\n",
    "    \"Restaurants and Food Services\": \"Encompassing a range of dining establishments, from fast food to fine dining, including take-out and delivery services.\"\n",
    "}\n",
    "\n",
    "# Create embeddings for enhanced category descriptions\n",
    "category_embeddings = [get_embedding(description) for description in category_descriptions.values()]\n",
    "\n",
    "# Directory containing JSON files\n",
    "data_directory = 'data/receipts/json/json_for_classification'\n",
    "json_files = [f for f in os.listdir(data_directory) if f.endswith('.json')]\n",
    "\n",
    "# Process JSON files and generate vendor embeddings\n",
    "vendor_data = []\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(data_directory, json_file), 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "        merchant_name = json_data['ReceiptInfo']['merchant']\n",
    "        address = json_data['ReceiptInfo']['address']\n",
    "        city = json_data['ReceiptInfo']['city']\n",
    "        state = json_data['ReceiptInfo']['state']\n",
    "        tax = json_data['ReceiptInfo']['tax']\n",
    "        receipt_time = json_data['ReceiptInfo']['receiptTime']\n",
    "        \n",
    "        # Skip vendors with \"<UNKNOWN>\" names\n",
    "        if merchant_name == \"<UNKNOWN>\":\n",
    "            continue\n",
    "        \n",
    "        items = json_data['ReceiptInfo']['ITEMS']\n",
    "        \n",
    "        # Create embeddings for merchant name, address, and city\n",
    "        merchant_embedding = get_embedding(merchant_name)\n",
    "#         address_embedding = get_embedding(address)\n",
    "#         city_embedding = get_embedding(city)\n",
    "        \n",
    "        # Create embeddings for \"description,\" \"quantity,\" \"unitPrice,\" and \"totalPrice\" within \"ITEMS\"\n",
    "        item_embeddings = []\n",
    "        for item in items:\n",
    "            description = item['description']\n",
    "            quantity = item['quantity']\n",
    "            unit_price = item['unitPrice']\n",
    "            total_price = item['totalPrice']\n",
    "            \n",
    "            item_details = f\"Description: {description}, Quantity: {quantity}, Unit Price: {unit_price}, Total Price: {total_price}\"\n",
    "            item_embedding = get_embedding(item_details)\n",
    "            item_embeddings.append(item_embedding)\n",
    "        \n",
    "        # Create embeddings for \"state,\" \"tax,\" and \"receiptTime\"\n",
    "        state_embedding = get_embedding(state)\n",
    "        tax_embedding = get_embedding(str(tax))\n",
    "        receipt_time_embedding = get_embedding(str(receipt_time))\n",
    "        \n",
    "        # Combine all embeddings for this vendor\n",
    "        combined_embedding = np.concatenate(\n",
    "            (merchant_embedding, address_embedding, city_embedding, state_embedding, tax_embedding, receipt_time_embedding, *item_embeddings),\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        vendor_data.append({'merchant': merchant_name, 'embedding': combined_embedding, \"file_name\": json_file})\n",
    "\n",
    "# Calculate cosine similarity and assign vendors to categories\n",
    "similarity_matrix = np.zeros((len(vendor_data), len(category_embeddings)))\n",
    "\n",
    "for i, vendor in enumerate(vendor_data):\n",
    "    combined_embedding = vendor['embedding']\n",
    "    for j, category_embedding in enumerate(category_embeddings):\n",
    "        # Ensure category_embedding has the same dimension as combined_embedding\n",
    "        if combined_embedding.shape[0] != category_embedding.shape[0]:\n",
    "            category_embedding = np.pad(category_embedding, (0, combined_embedding.shape[0] - category_embedding.shape[0]))\n",
    "        similarity = cosine_similarity([combined_embedding], [category_embedding])\n",
    "        similarity_matrix[i, j] = similarity[0, 0]\n",
    "\n",
    "# Assign vendors to categories based on highest similarity\n",
    "cluster_labels = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "# Create and export the result DataFrame\n",
    "result_df = pd.DataFrame(columns=['Category', 'Vendor Name', 'File Name'])\n",
    "for i, vendor in enumerate(vendor_data):\n",
    "    category = list(category_descriptions.keys())[cluster_labels[i]]\n",
    "    result_df = result_df.append({\n",
    "        'Category': category, \n",
    "        'Vendor Name': vendor['merchant'], \n",
    "        'File Name': vendor['file_name']\n",
    "    }, ignore_index=True)\n",
    "\n",
    "result_df.sort_values(by=['Category', 'Vendor Name'], inplace=True)\n",
    "result_df.to_csv('vendor_classification.csv', index=False)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8378d76-b6ae-4052-b304-38b0d252c235",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1ef27d1-7eb7-4228-9031-e26c353c8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.260334132346938\n",
      "Adjusted Rand Index: 0.24615384615384617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Calculate the silhouette score for the clustering result\n",
    "silhouette_avg = silhouette_score(similarity_matrix, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "ground_truth_mapping = {\n",
    "    \"18ac5f08-bceb-450f-9de2-85084af2fa13_prompttemplate2.json\": 7,\n",
    "    \"8e7e79a2-1397-4230-bf37-4792e53dac38_prompttemplate2.json\": 1,\n",
    "    \"5ca657d1-3e7f-4c13-b9f7-d03e29677db9_prompttemplate2.json\": 1,\n",
    "    \"4e00e015-80b1-482d-b153-6742be0fa1ce_prompttemplate2.json\": 2,\n",
    "    \"10c4a651-16c4-4721-99ac-a6d53e3f5656_prompttemplate2.json\": 3,\n",
    "    \"8c2864bb-b3cf-4c27-8b33-d8a525696ee2_prompttemplate2.json\": 2,\n",
    "    \"8b23d4b5-dbd4-4cf9-b9ba-b9834d07a7d7_prompttemplate2.json\": 5,\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a list of ground truth labels for the files with known labels\n",
    "ground_truth_labels = [ground_truth_mapping.get(json_file, -1) for json_file in json_files if json_file in ground_truth_mapping]\n",
    "\n",
    "# Create a list of cluster labels for the corresponding files\n",
    "cluster_labels_filtered = [cluster_labels[i] for i, json_file in enumerate(json_files) if json_file in ground_truth_mapping]\n",
    "\n",
    "# Calculate the adjusted Rand index\n",
    "ari = adjusted_rand_score(ground_truth_labels, cluster_labels_filtered)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c304cf-6052-4d94-bceb-67a2b82e6855",
   "metadata": {},
   "source": [
    "### K Mean and DBSCAN"
=======
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def try_read_file(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']  # Add more if needed\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return json.load(file)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(f\"File {file_path} has an unknown encoding.\")\n",
    "\n",
    "def json_to_string(data):\n",
    "    data_copy = data.copy()\n",
    "    data_copy['ReceiptInfo'].pop('merchantCategory', None)\n",
    "    return json.dumps(data_copy, sort_keys=True)\n",
    "\n",
    "# Path to the JSON files\n",
    "json_folder_path = 'data/receipts/json/prompt2'\n",
    "\n",
    "# Collect data for training\n",
    "data = {'json_string': [], 'category': []}\n",
    "\n",
    "# Iterate over JSON files\n",
    "for file_name in os.listdir(json_folder_path):\n",
    "    file_path = os.path.join(json_folder_path, file_name)\n",
    "    try:\n",
    "        data_json = try_read_file(file_path)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    merchant_category = data_json['ReceiptInfo'].get('merchantCategory')\n",
    "    \n",
    "    if not merchant_category:\n",
    "        continue\n",
    "\n",
    "    json_string = json_to_string(data_json)\n",
    "    data['json_string'].append(json_string)\n",
    "    data['category'].append(merchant_category)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=3, ngram_range=(1, 3), stop_words='english')\n",
    "X = vectorizer.fit_transform(df['json_string'])\n",
    "y = df['category']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],\n",
    "    'max_depth': [2, 4, 6, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 16],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "print(f\"Improved Random Forest Model Accuracy: {accuracy_best_rf}\")\n",
    "\n",
    "# # Predict and save results for all data\n",
    "# all_predictions = rf_model.predict(X)\n",
    "# results = pd.DataFrame({'Category': all_predictions, 'Vendor Name': df['json_string'], 'File Name': df.index})\n",
    "# results.sort_values(by=['Category', 'Vendor Name'], inplace=True)\n",
    "# results.to_csv('vendor_classification_results.csv', index=False)\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 16,
   "id": "fecfacb1-6599-456b-a8f5-9f50fda20c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def get_embedding(text, tokenizer, model, device):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens.to(device))\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def get_vendor_embeddings(data_directory, tokenizer, model, device, combined_embedding_length=10000):\n",
    "    vendor_data = []\n",
    "    json_files = [f for f in os.listdir(data_directory) if f.endswith('.json')]\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(os.path.join(data_directory, json_file), 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "            merchant_name = json_data['ReceiptInfo']['merchant']\n",
    "            address = json_data['ReceiptInfo']['address']\n",
    "            city = json_data['ReceiptInfo']['city']\n",
    "            state = json_data['ReceiptInfo']['state']\n",
    "            tax = json_data['ReceiptInfo']['tax']\n",
    "            receipt_time = json_data['ReceiptInfo']['receiptTime']\n",
    "\n",
    "            if merchant_name == \"<UNKNOWN>\":\n",
    "                continue\n",
    "\n",
    "            items = json_data['ReceiptInfo']['ITEMS']\n",
    "\n",
    "            merchant_embedding = get_embedding(merchant_name, tokenizer, model, device)\n",
    "            address_embedding = get_embedding(address, tokenizer, model, device)\n",
    "            city_embedding = get_embedding(city, tokenizer, model, device)\n",
    "            state_embedding = get_embedding(state, tokenizer, model, device)\n",
    "            tax_embedding = get_embedding(str(tax), tokenizer, model, device)\n",
    "            receipt_time_embedding = get_embedding(str(receipt_time), tokenizer, model, device)\n",
    "\n",
    "            item_embeddings = []\n",
    "            for item in items:\n",
    "                item_details = f\"{item['description']}, {item['quantity']}, {item['unitPrice']}, {item['totalPrice']}\"\n",
    "                item_embeddings.append(get_embedding(item_details, tokenizer, model, device))\n",
    "\n",
    "            # Concatenate all embeddings and pad/truncate to fixed length\n",
    "            combined_embedding = np.concatenate(\n",
    "                [merchant_embedding, address_embedding, city_embedding, state_embedding, tax_embedding, receipt_time_embedding] + item_embeddings,\n",
    "                axis=None\n",
    "            )\n",
    "            if len(combined_embedding) > combined_embedding_length:\n",
    "                combined_embedding = combined_embedding[:combined_embedding_length]\n",
    "            else:\n",
    "                combined_embedding = np.pad(combined_embedding, (0, combined_embedding_length - len(combined_embedding)), 'constant')\n",
    "\n",
    "            vendor_data.append({'merchant': merchant_name, 'embedding': combined_embedding, 'file_name': json_file})\n",
    "\n",
    "    return vendor_data\n",
    "\n",
    "# Predefined categories\n",
    "categories = [\n",
    "    \"Grocery and Supermarkets\",\n",
    "    \"Clothing and Apparel\",\n",
    "    \"Health and Beauty\",\n",
    "    \"Electronics and Appliances\",\n",
    "    \"Home and Garden\",\n",
    "    \"Entertainment and Leisure\",\n",
    "    \"Restaurants and Food Services\"\n",
    "]\n",
    "\n",
    "# Create category embeddings\n",
    "category_embeddings = [get_embedding(category, tokenizer, model, device) for category in categories]\n",
    "\n",
    "# Load embeddings and vendor details\n",
    "data_directory = 'data/receipts/json/json_for_classification'\n",
    "vendor_data = get_vendor_embeddings(data_directory, tokenizer, model, device)\n",
    "embeddings = np.array([vendor['embedding'] for vendor in vendor_data])\n",
    "\n",
    "# Normalize and reduce dimensions of embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "reduced_embeddings = pca.fit_transform(scaled_embeddings)\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=len(categories))  # Number of clusters as the number of categories\n",
    "kmeans.fit(reduced_embeddings)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Assign each vendor to a category\n",
    "for vendor, label in zip(vendor_data, kmeans_labels):\n",
    "    vendor['Category'] = categories[label]\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Category': [vendor['Category'] for vendor in vendor_data],\n",
    "    'Vendor Name': [vendor['merchant'] for vendor in vendor_data],\n",
    "    'File Name': [vendor['file_name'] for vendor in vendor_data]\n",
    "})\n",
    "\n",
    "result_df = result_df.sort_values(by=['Category', 'Vendor Name'])\n",
    "\n",
    "# Saving results to CSV\n",
    "result_df.to_csv('vendor_classification_kmean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5a0e80-3704-4b12-bf8d-5574536761e4",
=======
   "execution_count": 35,
   "id": "ebf353e5-03ad-4d5b-8e25-efcc0c8116c3",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Silhouette Score: 0.11876953393220901\n",
      "Adjusted Rand Index: 0.125\n"
=======
      "Expecting value: line 1 column 1 (char 0)\n",
      "Random Forest Model Accuracy: 0.8181818181818182\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Calculate the silhouette score for the clustering result\n",
    "silhouette_avg = silhouette_score(reduced_embeddings, kmeans_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Ground truth mapping for some files\n",
    "ground_truth_mapping = {\n",
    "    \"18ac5f08-bceb-450f-9de2-85084af2fa13_prompttemplate2.json\": 7,\n",
    "    \"8e7e79a2-1397-4230-bf37-4792e53dac38_prompttemplate2.json\": 1,\n",
    "    \"5ca657d1-3e7f-4c13-b9f7-d03e29677db9_prompttemplate2.json\": 1,\n",
    "    \"4e00e015-80b1-482d-b153-6742be0fa1ce_prompttemplate2.json\": 2,\n",
    "    \"10c4a651-16c4-4721-99ac-a6d53e3f5656_prompttemplate2.json\": 3,\n",
    "    \"8c2864bb-b3cf-4c27-8b33-d8a525696ee2_prompttemplate2.json\": 2,\n",
    "    \"8b23d4b5-dbd4-4cf9-b9ba-b9834d07a7d7_prompttemplate2.json\": 5\n",
    "}\n",
    "\n",
    "# Create a list of ground truth labels for the files with known labels\n",
    "ground_truth_labels = [ground_truth_mapping.get(json_file, -1) for json_file in json_files if json_file in ground_truth_mapping]\n",
    "\n",
    "# Extract indices of files with known labels\n",
    "indices = [i for i, json_file in enumerate(json_files) if json_file in ground_truth_mapping]\n",
    "\n",
    "# Create a list of cluster labels for the corresponding files\n",
    "cluster_labels_filtered = [kmeans_labels[i] for i in indices]\n",
    "\n",
    "# Calculate the adjusted Rand index\n",
    "ari = adjusted_rand_score(ground_truth_labels, cluster_labels_filtered)\n",
    "print(f\"Adjusted Rand Index: {ari}\")\n"
=======
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def try_read_file(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']  # Add more if needed\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return json.load(file)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(f\"File {file_path} has an unknown encoding.\")\n",
    "\n",
    "def json_to_string(data):\n",
    "    data_copy = data.copy()\n",
    "    data_copy['ReceiptInfo'].pop('merchantCategory', None)\n",
    "    return json.dumps(data_copy, sort_keys=True)\n",
    "\n",
    "# Path to the JSON files\n",
    "json_folder_path = 'data/receipts/json/prompt2'\n",
    "\n",
    "# Collect data for training\n",
    "data = {'file_name': [], 'vendor_name': [], 'json_string': [], 'category': []}\n",
    "\n",
    "# Iterate over JSON files\n",
    "for file_name in os.listdir(json_folder_path):\n",
    "    file_path = os.path.join(json_folder_path, file_name)\n",
    "    try:\n",
    "        data_json = try_read_file(file_path)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    merchant_category = data_json['ReceiptInfo'].get('merchantCategory')\n",
    "    \n",
    "    if not merchant_category:\n",
    "        continue\n",
    "\n",
    "    merchant_name = data_json['ReceiptInfo'].get('merchant', 'Unknown')\n",
    "    json_string = json_to_string(data_json)\n",
    "    data['file_name'].append(file_name)\n",
    "    data['vendor_name'].append(merchant_name)\n",
    "    data['json_string'].append(json_string)\n",
    "    data['category'].append(merchant_category)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=3, ngram_range=(1, 3), stop_words='english')\n",
    "X = vectorizer.fit_transform(df['json_string'])\n",
    "y = df['category']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=10)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_leaf=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy_rf}\")\n",
    "\n",
    "# Predict and save results for all data\n",
    "all_predictions = rf_model.predict(X)\n",
    "results = pd.DataFrame({'Category': all_predictions, 'Vendor Name': df['vendor_name'], 'File Name':  df['file_name']})\n",
    "results.sort_values(by=['Category', 'Vendor Name'], inplace=True)\n",
    "results.to_csv('vendor_classification_results.csv', index=False)\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "id": "f71a6604-fd34-4314-a053-998b1702bb7a",
=======
   "id": "d56dae87-088c-4f11-94c0-1e6786e357cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2545a-41a0-4dbb-afeb-80cff4e11d27",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
